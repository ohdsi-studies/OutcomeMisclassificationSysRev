
@ARTICLE{VanderWeele2012-ua,
  title    = "Results on differential and dependent measurement error of the
              exposure and the outcome using signed directed acyclic graphs",
  author   = "VanderWeele, Tyler J and Hern{\'a}n, Miguel A",
  abstract = "Measurement error in both the exposure and the outcome is a
              common problem in epidemiologic studies. Measurement errors in
              the exposure and the outcome are said to be independent of each
              other if the measured exposure and the measured outcome are
              statistically independent conditional on the true exposure and
              true outcome (and dependent otherwise). Measurement error is said
              to be nondifferential if measurement of the exposure does not
              depend on the true outcome conditional on the true exposure and
              vice versa; otherwise it is said to be differential. Few results
              on differential and dependent measurement error are available in
              the literature. Here the authors use formal rules governing
              associations on signed directed acyclic graphs (DAGs) to draw
              conclusions about the presence and sign of causal effects under
              differential and dependent measurement error. The authors apply
              these rules to 4 forms of measurement error: independent
              nondifferential, dependent nondifferential, independent
              differential, and dependent differential. For a binary exposure
              and outcome, the authors generalize Weinberg et al.'s (Am J
              Epidemiol. 1994;140(6):565-571) result for nondifferential
              measurement error on preserving the direction of a trend to
              settings which also allow measurement error in the outcome and to
              cases involving dependent and/or differential error.",
  journal  = "Am. J. Epidemiol.",
  volume   =  175,
  number   =  12,
  pages    = "1303--1310",
  month    =  jun,
  year     =  2012,
  language = "en"
}

@ARTICLE{Lash2014-jd,
  title    = "Good practices for quantitative bias analysis",
  author   = "Lash, Timothy L and Fox, Matthew P and MacLehose, Richard F and
              Maldonado, George and McCandless, Lawrence C and Greenland,
              Sander",
  abstract = "Quantitative bias analysis serves several objectives in
              epidemiological research. First, it provides a quantitative
              estimate of the direction, magnitude and uncertainty arising from
              systematic errors. Second, the acts of identifying sources of
              systematic error, writing down models to quantify them, assigning
              values to the bias parameters and interpreting the results combat
              the human tendency towards overconfidence in research results,
              syntheses and critiques and the inferences that rest upon them.
              Finally, by suggesting aspects that dominate uncertainty in a
              particular research result or topic area, bias analysis can guide
              efficient allocation of sparse research resources. The
              fundamental methods of bias analyses have been known for decades,
              and there have been calls for more widespread use for nearly as
              long. There was a time when some believed that bias analyses were
              rarely undertaken because the methods were not widely known and
              because automated computing tools were not readily available to
              implement the methods. These shortcomings have been largely
              resolved. We must, therefore, contemplate other barriers to
              implementation. One possibility is that practitioners avoid the
              analyses because they lack confidence in the practice of bias
              analysis. The purpose of this paper is therefore to describe what
              we view as good practices for applying quantitative bias analysis
              to epidemiological data, directed towards those familiar with the
              methods. We focus on answering questions often posed to those of
              us who advocate incorporation of bias analysis methods into
              teaching and research. These include the following. When is bias
              analysis practical and productive? How does one select the biases
              that ought to be addressed? How does one select a method to model
              biases? How does one assign values to the parameters of a bias
              model? How does one present and interpret a bias analysis?. We
              hope that our guide to good practices for conducting and
              presenting bias analyses will encourage more widespread use of
              bias analysis to estimate the potential magnitude and direction
              of biases, as well as the uncertainty in estimates potentially
              influenced by the biases.",
  journal  = "Int. J. Epidemiol.",
  volume   =  43,
  number   =  6,
  pages    = "1969--1985",
  month    =  dec,
  year     =  2014,
  keywords = "Epidemiological biases; analysis; best practice",
  language = "en"
}

@ARTICLE{Funk2014-bk,
  title    = "Misclassification in administrative claims data: quantifying the
              impact on treatment effect estimates",
  author   = "Funk, Michele Jonsson and Landi, Suzanne N",
  abstract = "Misclassification is present in nearly every epidemiologic study,
              yet is rarely quantified in analysis in favor of a focus on
              random error. In this review, we discuss past and present wisdom
              on misclassification and what measures should be taken to
              quantify this influential bias, with a focus on bias in
              pharmacoepidemiologic studies. To date, pharmacoepidemiology
              primarily utilizes data obtained from administrative claims, a
              rich source of prescription data but susceptible to bias from
              unobservable factors including medication sample use, medications
              filled but not taken, health conditions that are not reported in
              the administrative billing data, and inadequate capture of
              confounders. Due to the increasing focus on comparative
              effectiveness research, we provide a discussion of
              misclassification in the context of an active comparator,
              including a demonstration of treatment effects biased away from
              the null in the presence of nondifferential misclassification.
              Finally, we highlight recently developed methods to quantify bias
              and offer these methods as potential options for strengthening
              the validity and quantifying uncertainty of results obtained from
              pharmacoepidemiologic research.",
  journal  = "Curr Epidemiol Rep",
  volume   =  1,
  number   =  4,
  pages    = "175--185",
  month    =  dec,
  year     =  2014,
  keywords = "Bayesian bias analysis; administrative claims; comparative
              effectiveness; measurement error; misclassification; modified
              maximum likelihood; multiple imputation for measurement error;
              new user design; pharmacoepidemiology; probabilistic bias
              analysis; propensity score calibration; regression calibration",
  language = "en"
}

@ARTICLE{Chu2006-zo,
  title    = "Sensitivity analysis of misclassification: a graphical and a
              Bayesian approach",
  author   = "Chu, Haitao and Wang, Zhaojie and Cole, Stephen R and Greenland,
              Sander",
  abstract = "PURPOSE: Misclassification can produce bias in measures of
              association. Sensitivity analyses have been suggested to explore
              the impact of such bias, but do not supply formally justified
              interval estimates. METHODS: To account for exposure
              misclassification, recently developed Bayesian approaches were
              extended to incorporate prior uncertainty and correlation of
              sensitivity and specificity. Under nondifferential
              misclassification, a contour plot is used to depict relations
              among the corrected odds ratio, sensitivity, and specificity.
              RESULTS: Methods are illustrated by application to a case-control
              study of cigarette smoking and invasive pneumococcal disease
              while varying the distributional assumptions about sensitivity
              and specificity. Results are compared with those of conventional
              methods, which do not account for misclassification, and a
              sensitivity analysis, which assumes fixed sensitivity and
              specificity. CONCLUSION: By using Bayesian methods, investigators
              can incorporate uncertainty about misclassification into
              probabilistic inferences.",
  journal  = "Ann. Epidemiol.",
  volume   =  16,
  number   =  11,
  pages    = "834--841",
  month    =  nov,
  year     =  2006,
  language = "en"
}

@ARTICLE{Tian2018-zy,
  title    = "Evaluating large-scale propensity score performance through
              real-world and synthetic data experiments",
  author   = "Tian, Yuxi and Schuemie, Martijn J and Suchard, Marc A",
  abstract = "Background: Propensity score adjustment is a popular approach for
              confounding control in observational studies. Reliable frameworks
              are needed to determine relative propensity score performance in
              large-scale studies, and to establish optimal propensity score
              model selection methods. Methods: We detail a propensity score
              evaluation framework that includes synthetic and real-world data
              experiments. Our synthetic experimental design extends the
              'plasmode' framework and simulates survival data under known
              effect sizes, and our real-world experiments use a set of
              negative control outcomes with presumed null effect sizes. In
              reproductions of two published cohort studies, we compare two
              propensity score estimation methods that contrast in their model
              selection approach: L1-regularized regression that conducts a
              penalized likelihood regression, and the 'high-dimensional
              propensity score' (hdPS) that employs a univariate covariate
              screen. We evaluate methods on a range of outcome-dependent and
              outcome-independent metrics. Results: L1-regularization
              propensity score methods achieve superior model fit, covariate
              balance and negative control bias reduction compared with the
              hdPS. Simulation results are mixed and fluctuate with simulation
              parameters, revealing a limitation of simulation under the
              proportional hazards framework. Including regularization with the
              hdPS reduces commonly reported non-convergence issues but has
              little effect on propensity score performance. Conclusions:
              L1-regularization incorporates all covariates simultaneously into
              the propensity score model and offers propensity score
              performance superior to the hdPS marginal screen.",
  journal  = "Int. J. Epidemiol.",
  volume   =  47,
  number   =  6,
  pages    = "2005--2014",
  month    =  dec,
  year     =  2018,
  language = "en"
}

@ARTICLE{Franklin2017-fn,
  title    = "Comparing the performance of propensity score methods in
              healthcare database studies with rare outcomes",
  author   = "Franklin, Jessica M and Eddings, Wesley and Austin, Peter C and
              Stuart, Elizabeth A and Schneeweiss, Sebastian",
  abstract = "Nonrandomized studies of treatments from electronic healthcare
              databases are critical for producing the evidence necessary to
              making informed treatment decisions, but often rely on comparing
              rates of events observed in a small number of patients. In
              addition, studies constructed from electronic healthcare
              databases, for example, administrative claims data, often adjust
              for many, possibly hundreds, of potential confounders. Despite
              the importance of maximizing efficiency when there are many
              confounders and few observed outcome events, there has been
              relatively little research on the relative performance of
              different propensity score methods in this context. In this
              paper, we compare a wide variety of propensity-based estimators
              of the marginal relative risk. In contrast to prior research that
              has focused on specific statistical methods in isolation of other
              analytic choices, we instead consider a method to be defined by
              the complete multistep process from propensity score modeling to
              final treatment effect estimation. Propensity score model
              estimation methods considered include ordinary logistic
              regression, Bayesian logistic regression, lasso, and boosted
              regression trees. Methods for utilizing the propensity score
              include pair matching, full matching, decile strata, fine strata,
              regression adjustment using one or two nonlinear splines, inverse
              propensity weighting, and matching weights. We evaluate methods
              via a 'plasmode' simulation study, which creates simulated
              datasets on the basis of a real cohort study of two treatments
              constructed from administrative claims data. Our results suggest
              that regression adjustment and matching weights, regardless of
              the propensity score model estimation method, provide lower bias
              and mean squared error in the context of rare binary outcomes.
              Copyright \copyright{} 2017 John Wiley \& Sons, Ltd.",
  journal  = "Stat. Med.",
  volume   =  36,
  number   =  12,
  pages    = "1946--1963",
  month    =  may,
  year     =  2017,
  keywords = "epidemiology; healthcare databases; propensity score; risk ratio;
              simulation",
  language = "en"
}

@ARTICLE{Rubin1997-tp,
  title    = "Estimating causal effects from large data sets using propensity
              scores",
  author   = "Rubin, D B",
  abstract = "The aim of many analyses of large databases is to draw causal
              inferences about the effects of actions, treatments, or
              interventions. Examples include the effects of various options
              available to a physician for treating a particular patient, the
              relative efficacies of various health care providers, and the
              consequences of implementing a new national health care policy. A
              complication of using large databases to achieve such aims is
              that their data are almost always observational rather than
              experimental. That is, the data in most large data sets are not
              based on the results of carefully conducted randomized clinical
              trials, but rather represent data collected through the
              observation of systems as they operate in normal practice without
              any interventions implemented by randomized assignment rules.
              Such data are relatively inexpensive to obtain, however, and
              often do represent the spectrum of medical practice better than
              the settings of randomized experiments. Consequently, it is
              sensible to try to estimate the effects of treatments from such
              large data sets, even if only to help design a new randomized
              experiment or shed light on the generalizability of results from
              existing randomized experiments. However, standard methods of
              analysis using available statistical software (such as linear or
              logistic regression) can be deceptive for these objectives
              because they provide no warnings about their propriety.
              Propensity score methods are more reliable tools for addressing
              such objectives because the assumptions needed to make their
              answers appropriate are more assessable and transparent to the
              investigator.",
  journal  = "Ann. Intern. Med.",
  volume   =  127,
  number   = "8 Pt 2",
  pages    = "757--763",
  month    =  oct,
  year     =  1997,
  language = "en"
}

@ARTICLE{Austin2011-we,
  title    = "An Introduction to Propensity Score Methods for Reducing the
              Effects of Confounding in Observational Studies",
  author   = "Austin, Peter C",
  abstract = "The propensity score is the probability of treatment assignment
              conditional on observed baseline characteristics. The propensity
              score allows one to design and analyze an observational
              (nonrandomized) study so that it mimics some of the particular
              characteristics of a randomized controlled trial. In particular,
              the propensity score is a balancing score: conditional on the
              propensity score, the distribution of observed baseline
              covariates will be similar between treated and untreated
              subjects. I describe 4 different propensity score methods:
              matching on the propensity score, stratification on the
              propensity score, inverse probability of treatment weighting
              using the propensity score, and covariate adjustment using the
              propensity score. I describe balance diagnostics for examining
              whether the propensity score model has been adequately specified.
              Furthermore, I discuss differences between regression-based
              methods and propensity score-based methods for the analysis of
              observational data. I describe different causal average treatment
              effects and their relationship with propensity score analyses.",
  journal  = "Multivariate Behav. Res.",
  volume   =  46,
  number   =  3,
  pages    = "399--424",
  month    =  may,
  year     =  2011,
  language = "en"
}

@ARTICLE{Rosenbaum1983-hi,
  title     = "The central role of the propensity score in observational
               studies for causal effects",
  author    = "Rosenbaum, Paul R and Rubin, Donald B",
  abstract  = "Abstract. The propensity score is the conditional probability of
               assignment to a particular treatment given a vector of observed
               covariates. Both large and smal",
  journal   = "Biometrika",
  publisher = "Oxford Academic",
  volume    =  70,
  number    =  1,
  pages     = "41--55",
  month     =  apr,
  year      =  1983,
  language  = "en"
}

@ARTICLE{Stuart2015-gg,
  title    = "Assessing the generalizability of randomized trial results to
              target populations",
  author   = "Stuart, Elizabeth A and Bradshaw, Catherine P and Leaf, Philip J",
  abstract = "Recent years have seen increasing interest in and attention to
              evidence-based practices, where the ``evidence'' generally comes
              from well-conducted randomized trials. However, while those
              trials yield accurate estimates of the effect of the intervention
              for the participants in the trial (known as ``internal
              validity''), they do not always yield relevant information about
              the effects in a particular target population (known as
              ``external validity''). This may be due to a lack of
              specification of a target population when designing the trial,
              difficulties recruiting a sample that is representative of a
              prespecified target population, or to interest in considering a
              target population somewhat different from the population directly
              targeted by the trial. This paper first provides an overview of
              existing design and analysis methods for assessing and enhancing
              the ability of a randomized trial to estimate treatment effects
              in a target population. It then provides a case study using one
              particular method, which weights the subjects in a randomized
              trial to match the population on a set of observed
              characteristics. The case study uses data from a randomized trial
              of school-wide positive behavioral interventions and supports
              (PBIS); our interest is in generalizing the results to the state
              of Maryland. In the case of PBIS, after weighting, estimated
              effects in the target population were similar to those observed
              in the randomized trial. The paper illustrates that statistical
              methods can be used to assess and enhance the external validity
              of randomized trials, making the results more applicable to
              policy and clinical questions. However, there are also many open
              research questions; future research should focus on questions of
              treatment effect heterogeneity and further developing these
              methods for enhancing external validity. Researchers should think
              carefully about the external validity of randomized trials and be
              cautious about extrapolating results to specific populations
              unless they are confident of the similarity between the trial
              sample and that target population.",
  journal  = "Prev. Sci.",
  volume   =  16,
  number   =  3,
  pages    = "475--485",
  month    =  apr,
  year     =  2015,
  language = "en"
}

@ARTICLE{Sherman2016-yn,
  title    = "{Real-World} Evidence - What Is It and What Can It Tell Us?",
  author   = "Sherman, Rachel E and Anderson, Steven A and Dal Pan, Gerald J
              and Gray, Gerry W and Gross, Thomas and Hunter, Nina L and
              LaVange, Lisa and Marinac-Dabic, Danica and Marks, Peter W and
              Robb, Melissa A and Shuren, Jeffrey and Temple, Robert and
              Woodcock, Janet and Yue, Lilly Q and Califf, Robert M",
  journal  = "N. Engl. J. Med.",
  volume   =  375,
  number   =  23,
  pages    = "2293--2297",
  month    =  dec,
  year     =  2016,
  language = "en"
}

@BOOK{Lash2009-ch,
  title     = "Applying Quantitative Bias Analysis to Epidemiologic Data",
  author    = "Lash, Timothy L and Fox, Matthew P and Fink, Aliza K",
  abstract  = "Bias analysis quantifies the influence of systematic error on an
               epidemiology study's estimate of association. The fundamental
               methods of bias analysis in epi- miology have been well
               described for decades, yet are seldom applied in published
               presentations of epidemiologic research. More recent advances in
               bias analysis, such as probabilistic bias analysis, appear even
               more rarely. We suspect that there are both supply-side and
               demand-side explanations for the scarcity of bias analysis. On
               the demand side, journal reviewers and editors seldom request
               that authors address systematic error aside from listing them as
               limitations of their particular study. This listing is often
               accompanied by explanations for why the limitations should not
               pose much concern. On the supply side, methods for bias analysis
               receive little attention in most epidemiology curriculums, are
               often scattered throughout textbooks or absent from them
               altogether, and cannot be implemented easily using standard
               statistical computing software. Our objective in this text is to
               reduce these supply-side barriers, with the hope that demand for
               quantitative bias analysis will follow.",
  publisher = "Springer New York",
  month     =  may,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Petersen2021-yk,
  title    = "A systematic review of quantitative bias analysis applied to
              epidemiological research",
  author   = "Petersen, Julie M and Ranker, Lynsie R and Barnard-Mayers, Ruby
              and MacLehose, Richard F and Fox, Matthew P",
  abstract = "BACKGROUND: Quantitative bias analysis (QBA) measures study
              errors in terms of direction, magnitude and uncertainty. This
              systematic review aimed to describe how QBA has been applied in
              epidemiological research in 2006-19. METHODS: We searched PubMed
              for English peer-reviewed studies applying QBA to real-data
              applications. We also included studies citing selected sources or
              which were identified in a previous QBA review in
              pharmacoepidemiology. For each study, we extracted the rationale,
              methodology, bias-adjusted results and interpretation and
              assessed factors associated with reproducibility. RESULTS: Of the
              238 studies, the majority were embedded within papers whose main
              inferences were drawn from conventional approaches as secondary
              (sensitivity) analyses to quantity-specific biases (52\%) or to
              assess the extent of bias required to shift the point estimate to
              the null (25\%); 10\% were standalone papers. The most common
              approach was probabilistic (57\%). Misclassification was modelled
              in 57\%, uncontrolled confounder(s) in 40\% and selection bias in
              17\%. Most did not consider multiple biases or correlations
              between errors. When specified, bias parameters came from the
              literature (48\%) more often than internal validation studies
              (29\%). The majority (60\%) of analyses resulted in >10\% change
              from the conventional point estimate; however, most investigators
              (63\%) did not alter their original interpretation. Degree of
              reproducibility related to inclusion of code, formulas,
              sensitivity analyses and supplementary materials, as well as the
              QBA rationale. CONCLUSIONS: QBA applications were rare though
              increased over time. Future investigators should reference good
              practices and include details to promote transparency and to
              serve as a reference for other researchers.",
  journal  = "Int. J. Epidemiol.",
  month    =  apr,
  year     =  2021,
  keywords = "Epidemiologic bias; epidemiologic study characteristics;
              quantitative bias analysis; quantitative evaluation; systematic
              bias; uncertainty",
  language = "en"
}

@ARTICLE{Hunnicutt2016-gn,
  title    = "Probabilistic bias analysis in pharmacoepidemiology and
              comparative effectiveness research: a systematic review",
  author   = "Hunnicutt, Jacob N and Ulbricht, Christine M and
              Chrysanthopoulou, Stavroula A and Lapane, Kate L",
  abstract = "PURPOSE: We systematically reviewed pharmacoepidemiologic and
              comparative effectiveness studies that use probabilistic bias
              analysis to quantify the effects of systematic error including
              confounding, misclassification, and selection bias on study
              results. METHODS: We found articles published between 2010 and
              October 2015 through a citation search using Web of Science and
              Google Scholar and a keyword search using PubMed and Scopus.
              Eligibility of studies was assessed by one reviewer. Three
              reviewers independently abstracted data from eligible studies.
              RESULTS: Fifteen studies used probabilistic bias analysis and
              were eligible for data abstraction-nine simulated an unmeasured
              confounder and six simulated misclassification. The majority of
              studies simulating an unmeasured confounder did not specify the
              range of plausible estimates for the bias parameters. Studies
              simulating misclassification were in general clearer when
              reporting the plausible distribution of bias parameters.
              Regardless of the bias simulated, the probability distributions
              assigned to bias parameters, number of simulated iterations,
              sensitivity analyses, and diagnostics were not discussed in the
              majority of studies. CONCLUSION: Despite the prevalence and
              concern of bias in pharmacoepidemiologic and comparative
              effectiveness studies, probabilistic bias analysis to
              quantitatively model the effect of bias was not widely used. The
              quality of reporting and use of this technique varied and was
              often unclear. Further discussion and dissemination of the
              technique are warranted. Copyright \copyright{} 2016 John Wiley
              \& Sons, Ltd.",
  journal  = "Pharmacoepidemiol. Drug Saf.",
  volume   =  25,
  number   =  12,
  pages    = "1343--1353",
  month    =  dec,
  year     =  2016,
  keywords = "comparative effectiveness; pharmacoepidemiology; probabilistic
              bias analysis; quantitative bias analysis",
  language = "en"
}

@ARTICLE{Fox2005-vb,
  title    = "A method to automate probabilistic sensitivity analyses of
              misclassified binary variables",
  author   = "Fox, Matthew P and Lash, Timothy L and Greenland, Sander",
  abstract = "BACKGROUND: Misclassification bias is present in most studies,
              yet uncertainty about its magnitude or direction is rarely
              quantified. METHODS: The authors present a method for
              probabilistic sensitivity analysis to quantify likely effects of
              misclassification of a dichotomous outcome, exposure or
              covariate. This method involves reconstructing the data that
              would have been observed had the misclassified variable been
              correctly classified, given the sensitivity and specificity of
              classification. The accompanying SAS macro implements the method
              and allows users to specify ranges of sensitivity and specificity
              of misclassification parameters to yield simulation intervals
              that incorporate both systematic and random error. RESULTS: The
              authors illustrate the method and the accompanying SAS macro code
              by applying it to a study of the relation between occupational
              resin exposure and lung-cancer deaths. The authors compare the
              results using this method with the conventional result, which
              accounts for random error only, and with the original sensitivity
              analysis results. CONCLUSION: By accounting for plausible degrees
              of misclassification, investigators can present study results in
              a way that incorporates uncertainty about the bias due to
              misclassification, and so avoid misleadingly precise-looking
              results.",
  journal  = "Int. J. Epidemiol.",
  volume   =  34,
  number   =  6,
  pages    = "1370--1376",
  month    =  dec,
  year     =  2005,
  language = "en"
}

@ARTICLE{Lash2003-pe,
  title    = "Semi-automated sensitivity analysis to assess systematic errors
              in observational data",
  author   = "Lash, Timothy L and Fink, Aliza K",
  abstract = "BACKGROUND: Published epidemiologic research usually provides a
              quantitative assessment of random error for effect estimates, but
              no quantitative assessment of systematic error. Sensitivity
              analysis can provide such an assessment. METHODS: We describe a
              method to reconstruct epidemiologic data, accounting for biases,
              and to display the results of repeated reconstructions as an
              assessment of error. We illustrate with a study of the effect of
              less-than-definitive therapy on breast cancer mortality. RESULTS:
              We developed SAS code to reconstruct the data that would have
              been observed had a set of systematic errors been absent, and to
              convey the results. After 4,000 reconstructions of the example
              data, we obtained a median estimate of relative hazard equal to
              1.5 with a 95\% simulation interval of 0.8-2.8. The relative
              hazard obtained by conventional analysis equaled 2.0, with a 95\%
              confidence interval of 1.2-3.4. CONCLUSIONS: Our method of
              sensitivity analysis can be used to quantify the systematic error
              for an estimate of effect and to describe that error in figures,
              tables, or text. In the example, the sources of error biased the
              conventional relative hazard away from the null, and that error
              was not accurately communicated by the conventional confidence
              interval.",
  journal  = "Epidemiology",
  volume   =  14,
  number   =  4,
  pages    = "451--458",
  month    =  jul,
  year     =  2003,
  language = "en"
}

@ARTICLE{Greenland1996-hy,
  title    = "Basic methods for sensitivity analysis of biases",
  author   = "Greenland, S",
  abstract = "BACKGROUND: Most discussions of statistical methods focus on
              accounting for measured confounders and random errors in the
              data-generating process. In observational epidemiology, however,
              controllable confounding and random error are sometimes only a
              fraction of the total error, and are rarely if ever the only
              important source of uncertainty. Potential biases due to
              unmeasured confounders, classification errors, and selection bias
              need to be addressed in any thorough discussion of study results.
              METHODS: This paper reviews basic methods for examining the
              sensitivity of study results to biases, with a focus on methods
              that can be implemented without computer programming. CONCLUSION:
              Sensitivity analysis is helpful in obtaining a realistic picture
              of the potential impact of biases.",
  journal  = "Int. J. Epidemiol.",
  volume   =  25,
  number   =  6,
  pages    = "1107--1116",
  month    =  dec,
  year     =  1996,
  language = "en"
}

@ARTICLE{Greenland2003-zd,
  title    = "Quantifying biases in causal models: classical confounding vs
              collider-stratification bias",
  author   = "Greenland, Sander",
  abstract = "It has long been known that stratifying on variables affected by
              the study exposure can create selection bias. More recently it
              has been shown that stratifying on a variable that precedes
              exposure and disease can induce confounding, even if there is no
              confounding in the unstratified (crude) estimate. This paper
              examines the relative magnitudes of these biases under some
              simple causal models in which the stratification variable is
              graphically depicted as a collider (a variable directly affected
              by two or more other variables in the graph). The results suggest
              that bias from stratifying on variables affected by exposure and
              disease may often be comparable in size with bias from classical
              confounding (bias from failing to stratify on a common cause of
              exposure and disease), whereas other biases from collider
              stratification may tend to be much smaller.",
  journal  = "Epidemiology",
  volume   =  14,
  number   =  3,
  pages    = "300--306",
  month    =  may,
  year     =  2003,
  language = "en"
}

@ARTICLE{Lash2014-jd,
  title    = "Good practices for quantitative bias analysis",
  author   = "Lash, Timothy L and Fox, Matthew P and MacLehose, Richard F and
              Maldonado, George and McCandless, Lawrence C and Greenland,
              Sander",
  abstract = "Quantitative bias analysis serves several objectives in
              epidemiological research. First, it provides a quantitative
              estimate of the direction, magnitude and uncertainty arising from
              systematic errors. Second, the acts of identifying sources of
              systematic error, writing down models to quantify them, assigning
              values to the bias parameters and interpreting the results combat
              the human tendency towards overconfidence in research results,
              syntheses and critiques and the inferences that rest upon them.
              Finally, by suggesting aspects that dominate uncertainty in a
              particular research result or topic area, bias analysis can guide
              efficient allocation of sparse research resources. The
              fundamental methods of bias analyses have been known for decades,
              and there have been calls for more widespread use for nearly as
              long. There was a time when some believed that bias analyses were
              rarely undertaken because the methods were not widely known and
              because automated computing tools were not readily available to
              implement the methods. These shortcomings have been largely
              resolved. We must, therefore, contemplate other barriers to
              implementation. One possibility is that practitioners avoid the
              analyses because they lack confidence in the practice of bias
              analysis. The purpose of this paper is therefore to describe what
              we view as good practices for applying quantitative bias analysis
              to epidemiological data, directed towards those familiar with the
              methods. We focus on answering questions often posed to those of
              us who advocate incorporation of bias analysis methods into
              teaching and research. These include the following. When is bias
              analysis practical and productive? How does one select the biases
              that ought to be addressed? How does one select a method to model
              biases? How does one assign values to the parameters of a bias
              model? How does one present and interpret a bias analysis?. We
              hope that our guide to good practices for conducting and
              presenting bias analyses will encourage more widespread use of
              bias analysis to estimate the potential magnitude and direction
              of biases, as well as the uncertainty in estimates potentially
              influenced by the biases.",
  journal  = "Int. J. Epidemiol.",
  volume   =  43,
  number   =  6,
  pages    = "1969--1985",
  month    =  dec,
  year     =  2014,
  keywords = "Epidemiological biases; analysis; best practice",
  language = "en"
}